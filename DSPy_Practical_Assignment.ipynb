{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7dpA1cXF4ckKpUEhR+DPM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subham15-11/justfor/blob/main/DSPy_Practical_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8TBFld2Eoo2",
        "outputId": "67fc8adc-8cd2-45b5-b697-0b02d9eb2dcf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dspy-ai in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: dspy>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from dspy-ai) (3.0.4)\n",
            "Requirement already satisfied: backoff>=2.2 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (1.5.2)\n",
            "Requirement already satisfied: openai>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.9.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2025.11.3)\n",
            "Requirement already satisfied: orjson>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.11.5)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.32.4)\n",
            "Requirement already satisfied: optuna>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.6.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.12.3)\n",
            "Requirement already satisfied: magicattr>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.1.6)\n",
            "Requirement already satisfied: litellm>=1.64.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (1.80.10)\n",
            "Requirement already satisfied: diskcache>=5.6.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (5.6.3)\n",
            "Requirement already satisfied: json-repair>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.54.3)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (9.1.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (4.12.0)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (0.0.8)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (6.2.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.1.2)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (13.9.4)\n",
            "Requirement already satisfied: pillow>=10.1.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (11.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (2.0.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from dspy>=3.0.4->dspy-ai) (3.6.0)\n",
            "Requirement already satisfied: gepa==0.0.17 in /usr/local/lib/python3.12/dist-packages (from gepa[dspy]==0.0.17->dspy>=3.0.4->dspy-ai) (0.0.17)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.4->dspy-ai) (3.11)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->dspy>=3.0.4->dspy-ai) (4.15.0)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (8.3.1)\n",
            "Requirement already satisfied: fastuuid>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.14.0)\n",
            "Requirement already satisfied: grpcio<1.68.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.67.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (4.25.1)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.22.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.4->dspy-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.4->dspy-ai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=0.28.1->dspy>=3.0.4->dspy-ai) (1.3.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.17.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (6.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (2.0.44)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.4->dspy-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.4->dspy-ai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dspy>=3.0.4->dspy-ai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.4->dspy-ai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.4->dspy-ai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->dspy>=3.0.4->dspy-ai) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.4->dspy-ai) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.7.1->dspy>=3.0.4->dspy-ai) (2.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.22.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (1.3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.30.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy>=3.0.4->dspy-ai) (0.1.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy>=3.0.4->dspy-ai) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.64.0->dspy>=3.0.4->dspy-ai) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install dspy-ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "# print(dspy.__version__)"
      ],
      "metadata": {
        "id": "Qg08TrzwFuOJ"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.metadata\n",
        "print(importlib.metadata.version(\"dspy-ai\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdGC19aEGfy7",
        "outputId": "ab52a448-658c-48e5-d39c-b34c638b8c8d"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import copy\n",
        "from typing import List, Optional\n",
        "from typing import Literal, Dict, Union"
      ],
      "metadata": {
        "id": "-jjqWuceHAZd"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY=\"ak_2KH0D27yJ60F6sh4vA8Dy5fr7zz0L\"\n",
        "main_lm = dspy.LM(\"openai/LongCat-Flash-Chat\", api_key=API_KEY,api_base=\"https://api.longcat.chat/openai/v1\")\n",
        "\n",
        "dspy.settings.configure(lm=main_lm,adapter=dspy.XMLAdapter())"
      ],
      "metadata": {
        "id": "RVmaeKKDm9h7"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = dspy.Predict(\"question -> answer\")\n",
        "print(test(question=\"Reply with OK only\").answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_yoNF01nFtG",
        "outputId": "bfd55d94-a804-408a-e4b8-b04bf9f30a30"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple\n",
        "from pydantic import BaseModel, Field\n",
        "import requests"
      ],
      "metadata": {
        "id": "cLx3JEYLpkIC"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTITY + ATTRIBUTE EXTRACTION\n",
        "# code uses Pydantic models inside a DSPy Signature.\n",
        "# If the LLM returns broken JSON or the wrong format, DSPy's internal logic and the Pydantic catches it\n",
        "class EntityWithAttr(BaseModel):\n",
        "    entity: str = Field(description=\"the named entity\")\n",
        "    attr_type: str = Field(description=\"semantic type of the entity (e.g. Drug, Disease, Symptom, etc.)\")\n",
        "\n",
        "class ExtractEntities(dspy.Signature):\n",
        "    \"\"\"From the paragraph extract all relevant entities and their semantic attribute types.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"input paragraph\")\n",
        "    entities: List[EntityWithAttr] = dspy.OutputField(desc=\"list of entities and their attribute types\")"
      ],
      "metadata": {
        "id": "TYIHwfuqpKcL"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEDUPLICATOR\n",
        "# LLMs are probabilistic. They can make mistakes.\n",
        "class DeduplicateEntities(dspy.Signature):\n",
        "    \"\"\"Given a list of (entity, attr_type) decide which ones are duplicates.\n",
        "    Return a deduplicated list and a confidence that the remaining items are ALL distinct.\"\"\"\n",
        "    items: List[EntityWithAttr] = dspy.InputField(desc=\"batch of entities to deduplicate\")\n",
        "    deduplicated: List[EntityWithAttr] = dspy.OutputField(desc=\"deduplicated list\")\n",
        "    confidence: float = dspy.OutputField(\n",
        "        desc=\"confidence (0-1) that every item in deduplicated is semantically distinct\"\n",
        "    )\n",
        "\n",
        "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)\n",
        "\n",
        "def deduplicate_with_lm(\n",
        "    items: List[EntityWithAttr],\n",
        "    *,\n",
        "    batch_size: int = 10,\n",
        "    target_confidence: float = 0.9,\n",
        ") -> List[EntityWithAttr]:\n",
        "    \"\"\"\n",
        "    Recursively deduplicate using the LM with confidence loop.\n",
        "    \"\"\"\n",
        "    if not items:\n",
        "        return []\n",
        "\n",
        "    # helper to process one batch\n",
        "    def _process_batch(batch: List[EntityWithAttr]) -> List[EntityWithAttr]:\n",
        "        attempts = 0\n",
        "        while attempts < 3: # Safety break to prevent infinite loops\n",
        "        # The LLM is asked to deduplicate the list and rate its own confidence (0.0 to 1.0). If the confidence is below 0.9 (90%), the code rejects the answer and tries again (up to 3 times).\n",
        "            pred = dedup_predictor(items=batch)\n",
        "            if pred.confidence >= target_confidence:\n",
        "                return pred.deduplicated\n",
        "            attempts += 1\n",
        "        # If confidence never reached, return last attempt or original\n",
        "        return pred.deduplicated if 'pred' in locals() else batch\n",
        "\n",
        "    # split into batches and process\n",
        "    results = []\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        batch = items[i : i + batch_size]\n",
        "        results.extend(_process_batch(batch))\n",
        "    return results"
      ],
      "metadata": {
        "id": "rcMMNa39pteZ"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELATION EXTRACTION\n",
        "class Relation(BaseModel):\n",
        "    subj: str = Field(description=\"subject entity (exact string as in deduplicated list)\")\n",
        "    pred: str = Field(description=\"short predicate / relation phrase\")\n",
        "    obj:  str = Field(description=\"object entity (exact string as in deduplicated list)\")\n",
        "\n",
        "class ExtractRelations(dspy.Signature):\n",
        "    \"\"\"Given the original paragraph and a list of unique entities, extract all factual (subject, predicate, object) triples that are explicitly stated or clearly implied.\"\"\"\n",
        "    paragraph: str = dspy.InputField(desc=\"original paragraph\")\n",
        "    entities:  List[str] = dspy.InputField(desc=\"list of deduplicated entity strings\")\n",
        "    relations: List[Relation] = dspy.OutputField(desc=\"list of subject-predicate-object triples\")\n",
        "\n",
        "rel_predictor = dspy.ChainOfThought(ExtractRelations)"
      ],
      "metadata": {
        "id": "wd_OK4TxqHXC"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "eZeA6RUsqtO5"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MERMAID SERIALISER\n",
        "def triples_to_mermaid(triples: list[Relation], entity_list: list[str], max_label_len: int = 40) -> str:\n",
        "    \"\"\"Convert triples to a VALID Mermaid flowchart LR diagram.\"\"\"\n",
        "    entity_set = {e.strip().lower() for e in entity_list}\n",
        "    lines = [\"flowchart LR\"]\n",
        "\n",
        "    def _make_id(s: str) -> str:\n",
        "        # Create valid Mermaid node ID\n",
        "        return re.sub(r'[^a-zA-Z0-9]', '', s)\n",
        "\n",
        "    for t in triples:\n",
        "        subj_norm, obj_norm = t.subj.strip().lower(), t.obj.strip().lower()\n",
        "\n",
        "        # Logic to ensure we only graph valid entities\n",
        "        if obj_norm in entity_set and subj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred\n",
        "        elif obj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred # Keep original direction if valid\n",
        "        elif subj_norm in entity_set:\n",
        "            src, dst, lbl = t.subj, t.obj, t.pred\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        lbl = lbl.strip()\n",
        "        if len(lbl) > max_label_len:\n",
        "            lbl = lbl[:max_label_len - 3] + \"...\"\n",
        "\n",
        "        src_id, dst_id = _make_id(src), _make_id(dst)\n",
        "        if src_id and dst_id:\n",
        "             lines.append(f'    {src_id}[\"{src}\"] -->|{lbl}| {dst_id}[\"{dst}\"]')\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "-xJ1XdcOqjXr"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install beautifulsoup4 trafilatura"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1VIU0PAB5p9N",
        "outputId": "38b5dd4c-dfe1-40c3-fa36-001b4b2ca0b3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.12/dist-packages (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.11.12)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.4)\n",
            "Requirement already satisfied: courlan>=1.3.2 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (1.3.2)\n",
            "Requirement already satisfied: htmldate>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (1.9.4)\n",
            "Requirement already satisfied: justext>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.0.2)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (6.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.11.3)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "ctei4Ivhq2LG"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MANUAL OVERRIDES\n",
        "# ScienceDirect (URLs 3 and 7) is aggressively blocking the Google Colab IP address (Error 403)\n",
        "# it fails 1.Requests + BeautifulSoup method 2.trafilatura\n",
        "MANUAL_OVERRIDES = {\n",
        "    # URL 3\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\": \"\"\"\n",
        "    Ivermectin is a macrolide antiparasitic drug with a 16-membered ring that is widely used for the treatment of many parasitic diseases such as river blindness, elephantiasis and scabies. Satoshi ōmura and William C. Campbell won the 2015 Nobel Prize in Physiology or Medicine for the discovery of the excellent efficacy of ivermectin against parasitic diseases. Recently, ivermectin has been reported to inhibit the proliferation of several tumor cells by regulating multiple signaling pathways. This suggests that ivermectin may be an anticancer drug with great potential. Here, we reviewed the related mechanisms by which ivermectin inhibited the development of different cancers and promoted programmed cell death and discussed the prospects for the clinical application of ivermectin as an anticancer drug for neoplasm therapy.\n",
        "    \"\"\",\n",
        "\n",
        "    # URL 7:\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\": \"\"\"\n",
        "    There is a significant relationship between ambient temperature and mortality. In healthy individuals with no underlying co-morbid conditions, there is an efficient heat regulation system which enables the body to effectively handle thermal stress. However, in vulnerable groups, especially in elderly over the age of 65 years, infants and individuals with co-morbid cardiovascular and/or respiratory conditions, there is a deficiency in thermoregulation. When temperatures exceed a certain limit, being cold winter spells or heat waves, there is an increase in the number of deaths. In particular, it has been shown that at temperatures above 27 °C, the daily mortality rate increases more rapidly per degree rise compared to when it drops below 27 °C.\n",
        "    \"\"\"\n",
        "}"
      ],
      "metadata": {
        "id": "jtPHSZER7N3n"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import trafilatura\n",
        "\n",
        "def scrape_url(url, max_paragraphs=7):\n",
        "    # 1. Check Manual Override first\n",
        "    if url in MANUAL_OVERRIDES:\n",
        "        print(\"Using Manual Override text.\")\n",
        "        return MANUAL_OVERRIDES[url]\n",
        "\n",
        "    print(f\"Scraping...\")\n",
        "\n",
        "    # 2. Try Requests+BS4\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        if resp.status_code == 200:\n",
        "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "            paras = [p.text.strip() for p in soup.find_all(\"p\") if len(p.text) > 50]\n",
        "            if paras: return \" \".join(paras[:max_paragraphs])\n",
        "    except: pass\n",
        "\n",
        "    # 3. Try Trafilatura\n",
        "    try:\n",
        "        downloaded = trafilatura.fetch_url(url)\n",
        "        if downloaded:\n",
        "            text = trafilatura.extract(downloaded)\n",
        "            if text: return text[:4000]\n",
        "    except: pass\n",
        "\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "OkEvwTlmrVb2"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URLs to Scrape\n",
        "URLS = [\n",
        "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
        "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
        "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
        "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
        "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
        "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
        "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
        "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
        "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
        "]"
      ],
      "metadata": {
        "id": "1JACG8wVr2aJ"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UuSbjm8JsdBY"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN EXECUTION LOOP\n",
        "extractor = dspy.Predict(ExtractEntities)\n",
        "csv_rows = []\n",
        "\n",
        "print(\"Starting Processing Pipeline...\\n\")\n",
        "\n",
        "for i, url in enumerate(URLS):\n",
        "    print(f\"[{i+1}/10] Processing: {url}\")\n",
        "    text = scrape_url(url)\n",
        "\n",
        "    if not text:\n",
        "        print(\"Empty text or scrape error. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # A. Extract\n",
        "    try:\n",
        "        pred_extract = extractor(paragraph=text)\n",
        "        raw_entities = pred_extract.entities\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction error: {e}\")\n",
        "        continue\n",
        "\n",
        "    if not raw_entities:\n",
        "        print(\"No entities found.\")\n",
        "        continue\n",
        "\n",
        "    # B. Deduplicate\n",
        "    unique_entities_objs = deduplicate_with_lm(raw_entities)\n",
        "    unique_entity_strings = [e.entity for e in unique_entities_objs]\n",
        "\n",
        "    # Add to CSV Data\n",
        "    seen_tags = set()\n",
        "    for e in unique_entities_objs:\n",
        "        if e.entity not in seen_tags:\n",
        "            csv_rows.append({\"link\": url, \"tag\": e.entity, \"tag_type\": e.attr_type})\n",
        "            seen_tags.add(e.entity)\n",
        "\n",
        "    # C. Extract Relations\n",
        "    try:\n",
        "        pred_rel = rel_predictor(paragraph=text, entities=unique_entity_strings)\n",
        "        triples = pred_rel.relations\n",
        "    except Exception as e:\n",
        "        print(f\"Relation error: {e}\")\n",
        "        triples = []\n",
        "\n",
        "    # D. Generate Mermaid\n",
        "    mermaid_content = triples_to_mermaid(triples, unique_entity_strings)\n",
        "    filename = f\"mermaid_{i+1}.md\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(mermaid_content)\n",
        "    print(f\"Saved {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PloFMlKdrf6e",
        "outputId": "b06d6dbb-e286-40a5-d1ca-5ef4bc980e96"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Processing Pipeline...\n",
            "\n",
            "[1/10] Processing: https://en.wikipedia.org/wiki/Sustainable_agriculture\n",
            "Scraping...\n",
            "Saved mermaid_1.md\n",
            "[2/10] Processing: https://www.nature.com/articles/d41586-025-03353-5\n",
            "Scraping...\n",
            "Saved mermaid_2.md\n",
            "[3/10] Processing: https://www.sciencedirect.com/science/article/pii/S1043661820315152\n",
            "Using Manual Override text.\n",
            "Saved mermaid_3.md\n",
            "[4/10] Processing: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\n",
            "Scraping...\n",
            "Saved mermaid_4.md\n",
            "[5/10] Processing: https://www.fao.org/3/y4671e/y4671e06.htm\n",
            "Scraping...\n",
            "Saved mermaid_5.md\n",
            "[6/10] Processing: https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\n",
            "Scraping...\n",
            "Saved mermaid_6.md\n",
            "[7/10] Processing: https://www.sciencedirect.com/science/article/pii/S0378378220307088\n",
            "Using Manual Override text.\n",
            "Saved mermaid_7.md\n",
            "[8/10] Processing: https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\n",
            "Scraping...\n",
            "Saved mermaid_8.md\n",
            "[9/10] Processing: https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\n",
            "Scraping...\n",
            "Saved mermaid_9.md\n",
            "[10/10] Processing: https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\n",
            "Scraping...\n",
            "Saved mermaid_10.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(csv_rows).drop_duplicates(subset=['link', 'tag'])\n",
        "df.to_csv(\"tags.csv\", index=False)\n",
        "print(\"\\nPipeline Complete!\")\n",
        "print(\"Generated files:\")\n",
        "print(\" tags.csv\")\n",
        "print(f\"mermaid_1.md ... mermaid_{len(URLS)}.md\")\n",
        "print(\"\\nSample CSV Data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHjfPp8htzfP",
        "outputId": "9e799491-8236-4ad1-a2f5-f349c3dac73f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline Complete!\n",
            "Generated files:\n",
            " tags.csv\n",
            "mermaid_1.md ... mermaid_10.md\n",
            "\n",
            "Sample CSV Data:\n",
            "                                                link  \\\n",
            "0  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "1  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "2  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "3  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "4  https://en.wikipedia.org/wiki/Sustainable_agri...   \n",
            "\n",
            "                        tag             tag_type  \n",
            "0   sustainable agriculture              Concept  \n",
            "1        ecosystem services              Concept  \n",
            "2            climate change  Environmental Issue  \n",
            "3  greenhouse gas emissions  Environmental Issue  \n",
            "4            water scarcity  Environmental Issue  \n"
          ]
        }
      ]
    }
  ]
}